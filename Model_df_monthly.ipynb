{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0f30e5-c3b6-4f8c-9e96-96a3232056d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a8f59f-5a61-4a29-9d06-964aed334350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>county_code</th>\n",
       "      <th>monthly_adj_deaths</th>\n",
       "      <th>monthly_crude_rate_10k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>OK</td>\n",
       "      <td>Adair</td>\n",
       "      <td>40001</td>\n",
       "      <td>16</td>\n",
       "      <td>7.245720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>OK</td>\n",
       "      <td>Adair</td>\n",
       "      <td>40001</td>\n",
       "      <td>20</td>\n",
       "      <td>9.057151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>OK</td>\n",
       "      <td>Adair</td>\n",
       "      <td>40001</td>\n",
       "      <td>15</td>\n",
       "      <td>6.792863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>OK</td>\n",
       "      <td>Adair</td>\n",
       "      <td>40001</td>\n",
       "      <td>14</td>\n",
       "      <td>6.340005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>OK</td>\n",
       "      <td>Adair</td>\n",
       "      <td>40001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month        date state county  county_code  monthly_adj_deaths  \\\n",
       "0  2018      1  2018-01-01    OK  Adair        40001                  16   \n",
       "1  2018      2  2018-02-01    OK  Adair        40001                  20   \n",
       "2  2018      3  2018-03-01    OK  Adair        40001                  15   \n",
       "3  2018      4  2018-04-01    OK  Adair        40001                  14   \n",
       "4  2018      5  2018-05-01    OK  Adair        40001                   1   \n",
       "\n",
       "   monthly_crude_rate_10k  \n",
       "0                7.245720  \n",
       "1                9.057151  \n",
       "2                6.792863  \n",
       "3                6.340005  \n",
       "4                0.452858  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "raw = pd.read_csv('df_monthly.csv')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f1741c-ae42-48f4-988c-b9a479c65b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 15888\n",
      "number of counties: 323\n"
     ]
    }
   ],
   "source": [
    "# number of data points\n",
    "print('number of data points:', raw.shape[0])\n",
    "# number of counties\n",
    "print('number of counties:', raw['county'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046cd5ad-788c-453e-a332-044ba38b3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type of each feature before converting:\n",
      " year                        int64\n",
      "month                       int64\n",
      "date                       object\n",
      "state                      object\n",
      "county                     object\n",
      "county_code                 int64\n",
      "monthly_adj_deaths          int64\n",
      "monthly_crude_rate_10k    float64\n",
      "dtype: object\n",
      "data type of each feature after converting:\n",
      " year                               int64\n",
      "month                              int64\n",
      "date                      datetime64[ns]\n",
      "state                             string\n",
      "county                            string\n",
      "county_code                        int64\n",
      "monthly_adj_deaths                 int64\n",
      "monthly_crude_rate_10k           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# convert features into correct dtype\n",
    "print('data type of each feature before converting:\\n',raw.dtypes)\n",
    "raw[['state','county']] = raw[['state','county']].astype('string')\n",
    "raw[['date']] = raw[['date']].astype('datetime64')\n",
    "print('data type of each feature after converting:\\n',raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2686b55-e67d-4f20-9be6-a283d656765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'month', 'date', 'state', 'county', 'county_code', 'death', 'death_rate']\n"
     ]
    }
   ],
   "source": [
    "# rename feature names for simplicity\n",
    "raw.rename(columns = {'monthly_adj_deaths':'death','monthly_crude_rate_10k':'death_rate'}, inplace=True)\n",
    "# print new feature names\n",
    "print(raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d850e1-c0aa-4d35-a87f-b11f10722ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort 'raw' by 'county_code' and 'year'\n",
    "raw = raw.sort_values(by=['county_code','year'])\n",
    "raw = raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e200570-90c0-498e-b517-931d452712f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                          2021\n",
      "month                           12\n",
      "date           2021-12-01 00:00:00\n",
      "state                           OK\n",
      "county                       Adair\n",
      "county_code                  40001\n",
      "death                           16\n",
      "death_rate                8.241475\n",
      "Name: 47, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(raw.iloc[47,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bddc60e-eefd-4e7c-b29f-90136017315c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db451cb-d98e-4baf-951c-8ee0525bc10f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5fe5a4-bb03-4271-90c5-4e2f6379e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month       date state county  county_code  death  death_rate  \\\n",
      "0  2018      1 2018-01-01    OK  Adair        40001     16    7.245720   \n",
      "1  2018      2 2018-02-01    OK  Adair        40001     20    9.057151   \n",
      "2  2018      3 2018-03-01    OK  Adair        40001     15    6.792863   \n",
      "3  2018      4 2018-04-01    OK  Adair        40001     14    6.340005   \n",
      "4  2018      5 2018-05-01    OK  Adair        40001      1    0.452858   \n",
      "\n",
      "   death_minmax  death_standard  \n",
      "0      0.008663       -0.181936  \n",
      "1      0.010828       -0.146345  \n",
      "2      0.008121       -0.190834  \n",
      "3      0.007580       -0.199732  \n",
      "4      0.000541       -0.315402  \n"
     ]
    }
   ],
   "source": [
    "# scale the 'death' before feeding it into LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "minmaxscaler = MinMaxScaler()\n",
    "standardscaler = StandardScaler()\n",
    "raw['death_minmax'] = minmaxscaler.fit_transform(raw[['death']])\n",
    "raw['death_standard'] = standardscaler.fit_transform(raw[['death']])\n",
    "# print first five rows to check scaling\n",
    "print(raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f598f86-6124-4d7a-b6ce-3b7617d2458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15888, 331)\n"
     ]
    }
   ],
   "source": [
    "# use onehot encoding to encode the feature 'county'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder(sparse_output = False)\n",
    "county_code_onehot = onehot.fit_transform(raw[['county_code']])\n",
    "print(county_code_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89ede8e-9c5e-4987-88e9-9f338951e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "death_minmax_train_lstm.shape: (15557, 1)\n",
      "death_minmax_test_lstm.shape: (1324, 1)\n",
      "death_standard_train_lstm.shape: (15557, 1)\n",
      "death_standard_test_lstm.shape: (1324, 1)\n"
     ]
    }
   ],
   "source": [
    "# split the 'raw' data into training set (2018.01 - 2021.11) and testing set (2021.09-2021.12)\n",
    "# set the start and end dates for training set\n",
    "start_date_train_lstm = pd.to_datetime('2018-01-01')\n",
    "end_date_train_lstm = pd.to_datetime('2021-11-01')\n",
    "raw_train_lstm = raw[(raw['date'] >= start_date_train_lstm) & (raw['date'] <= end_date_train_lstm)]\n",
    "print(len(raw_train_lstm) == (12*4-1)*county_code_onehot.shape[1])\n",
    "\n",
    "#set the start and end dates for testing set\n",
    "start_date_test_lstm = pd.to_datetime('2021-09-01')\n",
    "end_date_test_lstm = pd.to_datetime('2021-12-01')\n",
    "raw_test_lstm = raw[(raw['date'] >= start_date_test_lstm) & (raw['date'] <= end_date_test_lstm)]\n",
    "print(len(raw_test_lstm) == 4*county_code_onehot.shape[1])\n",
    "\n",
    "#convert the training set and testing set into numpy array of shape (row, column)\n",
    "death_minmax_train_lstm = np.array(raw_train_lstm['death_minmax'])\n",
    "death_minmax_train_lstm = death_minmax_train_lstm.reshape(-1,1)\n",
    "print('death_minmax_train_lstm.shape:',death_minmax_train_lstm.shape)\n",
    "death_minmax_test_lstm = np.array(raw_test_lstm['death_minmax'])\n",
    "death_minmax_test_lstm = death_minmax_test_lstm.reshape(-1,1)\n",
    "print('death_minmax_test_lstm.shape:',death_minmax_test_lstm.shape)\n",
    "\n",
    "death_standard_train_lstm = np.array(raw_train_lstm['death_minmax'])\n",
    "death_standard_train_lstm = death_standard_train_lstm.reshape(-1,1)\n",
    "print('death_standard_train_lstm.shape:',death_standard_train_lstm.shape)\n",
    "death_standard_test_lstm = np.array(raw_test_lstm['death_minmax'])\n",
    "death_standard_test_lstm = death_standard_test_lstm.reshape(-1,1)\n",
    "print('death_standard_test_lstm.shape:',death_standard_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bcb105-011c-40d5-a8fb-e99aa5d4d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a county_code, find its onehot vector representation\n",
    "def get_county_code_encoding(county_code):\n",
    "    county_code_list = raw['county_code'].tolist()\n",
    "    if county_code in county_code_list:\n",
    "        county_index = county_code_list.index(county_code)\n",
    "        county_encoding = county_code_onehot[county_index]\n",
    "        return county_encoding\n",
    "    else:\n",
    "        print('County code not found.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44db9455-6a20-48be-81d4-934316df362a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county_code_onehot_train_lstm.shape: (15557, 331)\n",
      "county_code_onehot_test_lstm.shape: (1324, 331)\n"
     ]
    }
   ],
   "source": [
    "# create the county feature for training set and testing set\n",
    "county_code_train_lstm = raw_train_lstm['county_code']\n",
    "county_code_onehot_train_lstm = county_code_train_lstm.map(get_county_code_encoding)\n",
    "county_code_onehot_train_lstm = np.vstack(county_code_onehot_train_lstm)\n",
    "print('county_code_onehot_train_lstm.shape:', county_code_onehot_train_lstm.shape)\n",
    "\n",
    "county_code_test_lstm = raw_test_lstm['county_code']\n",
    "county_code_onehot_test_lstm = county_code_test_lstm.map(get_county_code_encoding)\n",
    "county_code_onehot_test_lstm = np.vstack(county_code_onehot_test_lstm)\n",
    "print('county_code_onehot_test_lstm.shape:', county_code_onehot_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbda671e-afe6-40c8-9888-2740c7f19c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_minmax_train.shape: (15557, 332)\n",
      "data_minmax_test.shape: (1324, 332)\n",
      "data_standard_train.shape: (15557, 332)\n",
      "data_standard_test.shape: (1324, 332)\n"
     ]
    }
   ],
   "source": [
    "# horizontally stack the county feature and the death feature\n",
    "data_minmax_train = np.hstack((county_code_onehot_train_lstm,death_minmax_train_lstm))\n",
    "print('data_minmax_train.shape:',data_minmax_train.shape)\n",
    "data_minmax_test = np.hstack((county_code_onehot_test_lstm,death_minmax_test_lstm))\n",
    "print('data_minmax_test.shape:',data_minmax_test.shape)\n",
    "\n",
    "data_standard_train = np.hstack((county_code_onehot_train_lstm,death_standard_train_lstm))\n",
    "print('data_standard_train.shape:',data_standard_train.shape)\n",
    "data_standard_test = np.hstack((county_code_onehot_test_lstm,death_standard_test_lstm))\n",
    "print('data_standard_test.shape:',data_standard_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c6bec5b-da3a-4c8f-bb0f-076051e25a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequence(sequence, n_timestep): # n_timestep is the window size\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i+ n_timestep # find the end of the pattern\n",
    "        if end_ix > len(sequence)-1: # check if we are beyond the dataset\n",
    "            break\n",
    "        seq_x = sequence[i:end_ix, :]\n",
    "        seq_y = sequence[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267a8ae5-6bbf-4f1b-82d4-f7951fc959f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a two dimensional array of each county and death number, calcaute a sequence for each county and concatenate them together\n",
    "def get_input_sequence(array, n_timestep):\n",
    "    X_list, y_list = list(), list() #empty list to hold the sequence of each county\n",
    "    for i in range(raw['county_code'].nunique()):\n",
    "        n_obs_each_county = int(len(array)/raw['county_code'].nunique()) # number of observations for each county\n",
    "        begin_index = n_obs_each_county*i\n",
    "        end_index = begin_index + n_obs_each_county\n",
    "        subset_array = array[begin_index:end_index,] # split the array into each county to create sequence for each county\n",
    "        X_county, y_county = split_sequence(subset_array, n_timestep)\n",
    "        X_list.append(X_county)\n",
    "        y_list.append(y_county)\n",
    "    # vertically stack X_county\n",
    "    X = np.vstack(X_list)\n",
    "    # concatenate y_county\n",
    "    y = np.concatenate(y_list)\n",
    "    return X, y     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f5beabc-0ca1-4870-b8ca-370042369d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_minmax_train_lstm.shape: (14564, 3, 332)\n",
      "y_minmax_train_lstm.shape: (14564,)\n",
      "X_minmax_test_lstm.shape: (331, 3, 332)\n",
      "y_minmax_test_lstm.shape: (331,)\n",
      "\n",
      "X_standard_train_lstm.shape: (14564, 3, 332)\n",
      "y_standard_train_lstm.shape: (14564,)\n",
      "X_standard_test_lstm.shape: (331, 3, 332)\n",
      "y_standard_test_lstm.shape: (331,)\n"
     ]
    }
   ],
   "source": [
    "#If X_train has shape (num_samples, n_timestep, n_features), \n",
    "#then X_test should also have shape (num_test_samples, n_timestep, n_features)**\n",
    "X_minmax_train_lstm, y_minmax_train_lstm = get_input_sequence(array = data_minmax_train, n_timestep = 3)\n",
    "print('X_minmax_train_lstm.shape:',X_minmax_train_lstm.shape)\n",
    "print('y_minmax_train_lstm.shape:',y_minmax_train_lstm.shape)\n",
    "X_minmax_test_lstm, y_minmax_test_lstm = get_input_sequence(array = data_minmax_test, n_timestep = 3)\n",
    "print('X_minmax_test_lstm.shape:', X_minmax_test_lstm.shape)\n",
    "print('y_minmax_test_lstm.shape:', y_minmax_test_lstm.shape)\n",
    "print('')\n",
    "\n",
    "X_standard_train_lstm, y_standard_train_lstm = get_input_sequence(array = data_standard_train, n_timestep = 3)\n",
    "print('X_standard_train_lstm.shape:',X_standard_train_lstm.shape)\n",
    "print('y_standard_train_lstm.shape:',y_standard_train_lstm.shape)\n",
    "X_standard_test_lstm, y_standard_test_lstm = get_input_sequence(array = data_standard_test, n_timestep = 3)\n",
    "print('X_standard_test_lstm.shape:', X_standard_test_lstm.shape)\n",
    "print('y_standard_test_lstm.shape:', y_standard_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7275dd37-9316-44e2-a695-77df8b0b70eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 332\n"
     ]
    }
   ],
   "source": [
    "n_timestep = X_minmax_train_lstm.shape[1]\n",
    "n_features = X_minmax_train_lstm.shape[2]\n",
    "print(n_timestep, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e2c09-e6f7-415f-9e04-942d661bfe0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe4c4e9-5f1a-4eac-aa42-77ca4f899284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Use for loop to add LSTM layers according to the choice\n",
    "        for i in range(hp.Int('n_layers', 1, 6)):  # Here, we are allowing 1 to 3 LSTM layers\n",
    "            if i == 0:\n",
    "                # first layer specifies input_shape and returns sequences if there are more layers to follow\n",
    "                model.add(layers.LSTM(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
    "                                      activation='relu',\n",
    "                                      input_shape=(self.input_shape), \n",
    "                                      return_sequences=True if hp.Int('n_layers', 1, 3) > 1 else False)) \n",
    "            else:\n",
    "                # subsequent layers return sequences if they are not the last one\n",
    "                model.add(layers.LSTM(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
    "                                      activation='relu',\n",
    "                                      return_sequences=True if i < hp.Int('n_layers', 1, 3) - 1 else False)) \n",
    "\n",
    "        model.add(layers.Dense(1,))\n",
    "\n",
    "        model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])),\n",
    "                      loss='mse')\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71c075-6a19-46bf-9710-8ce1e945b4e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81bfeea4-7096-4aca-96ae-b5cced2a82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from random_search_lstm/tuning_lstm/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 512)               1730560   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1731073 (6.60 MB)\n",
      "Trainable params: 1731073 (6.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "input_shape = (n_timestep, n_features)\n",
    "\n",
    "hypermodel = LSTMHyperModel(input_shape)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=30,  # total number of hypter-parameter combinations\n",
    "    executions_per_trial=3, # for each combination, how many trails to traint to average the result\n",
    "    directory='random_search_lstm',\n",
    "    project_name='tuning_lstm'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10) # the validation loss does not improve for 10 epochs\n",
    "\n",
    "random.seed(9999)\n",
    "np.random.seed(9999)\n",
    "tf.random.set_seed(9999)\n",
    "\n",
    "tuner.search(X_minmax_train_lstm, y_minmax_train_lstm, \n",
    "             epochs=50,\n",
    "             validation_split=0.2,  # or use a separate validation set\n",
    "             callbacks=[early_stopping]) #This callback will be applied to each trial of the hyperparameter search\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "best_lstm_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "best_lstm_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "best_lstm_model.fit(X_minmax_train_lstm, y_minmax_train_lstm, epochs=50, callbacks=[early_stopping], verbose=0)\n",
    "best_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da003732-749e-4ea7-b73b-4edf404c636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 1s 2ms/step - loss: 3.1786e-05\n",
      "mse_minmax_train_lstm_scaled: 3.178596671205014e-05\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 4.2090e-05\n",
      "mse_minmax_test_lstm_scaled: 4.209001417621039e-05\n"
     ]
    }
   ],
   "source": [
    "# calcuate the MSE for training set and testing set on scaled data\n",
    "mse_minmax_train_lstm_scaled = best_lstm_model.evaluate(X_minmax_train_lstm, y_minmax_train_lstm)\n",
    "print('mse_minmax_train_lstm_scaled:', mse_minmax_train_lstm_scaled)\n",
    "mse_minmax_test_lstm_scaled = best_lstm_model.evaluate(X_minmax_test_lstm, y_minmax_test_lstm)\n",
    "print('mse_minmax_test_lstm_scaled:', mse_minmax_test_lstm_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc1eda-86cb-4451-b9ce-beca1a77c3bd",
   "metadata": {},
   "source": [
    "__*We can exclude the possibility of overfitting*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34bbf08e-cc48-4fc0-be17-87a8be1ec484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step\n",
      "mse_minmax_test_lstm = 13572.338563612746\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE of the test set on the orignal scale\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_predicted_minmax_test_lstm = minmaxscaler.inverse_transform(best_lstm_model.predict(X_minmax_test_lstm))\n",
    "y_predicted_minmax_test_lstm = y_predicted_minmax_test_lstm.reshape(y_predicted_minmax_test_lstm.shape[0])\n",
    "y_true_test = raw.loc[(raw['month'] == 12) & (raw['year'] == 2021)]['death'].values\n",
    "mse_standard_test_lstm = mean_squared_error(y_true_test, y_predicted_minmax_test_lstm)\n",
    "print('mse_minmax_test_lstm =', mse_minmax_test_lstm) #unbiased point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f98fb61f-63b6-4c09-bce9-2aa911b4d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6933835\n",
      "0\n",
      "1333.1964\n",
      "1399\n"
     ]
    }
   ],
   "source": [
    "print(min(y_predicted_minmax_test_lstm))\n",
    "print(min(y_true_test))\n",
    "print(max(y_predicted_minmax_test_lstm))\n",
    "print(max(y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d408e-cfcb-4473-9f1e-ca7294bf5bd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334d4ac7-4ac5-435f-9b6c-df8a49bde1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 13m 57s]\n",
      "val_loss: 0.0018580974234888952\n",
      "\n",
      "Best val_loss So Far: 0.0008871587439595411\n",
      "Total elapsed time: 03h 07m 53s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 3, 384)            1101312   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 3, 224)            545664    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 3, 384)            935424    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 3, 416)            1332864   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 64)                123136    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4038465 (15.41 MB)\n",
      "Trainable params: 4038465 (15.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "input_shape = (n_timestep, n_features)\n",
    "\n",
    "hypermodel = LSTMHyperModel(input_shape)\n",
    "\n",
    "tuner_std = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=30,  # total number of hypter-parameter combinations\n",
    "    executions_per_trial=3, # for each combination, how many trails to traint to average the result\n",
    "    directory='random_search_lstm_std',\n",
    "    project_name='tuning_lstm_std'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10) # the validation loss does not improve for 10 epochs\n",
    "\n",
    "random.seed(9999)\n",
    "np.random.seed(9999)\n",
    "tf.random.set_seed(9999)\n",
    "\n",
    "tuner_std.search(X_standard_train_lstm, y_standard_train_lstm, \n",
    "             epochs=50,\n",
    "             validation_split=0.2,  # or use a separate validation set\n",
    "             callbacks=[early_stopping]) #This callback will be applied to each trial of the hyperparameter search\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps_std = tuner_std.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "best_lstm_model_std = tuner_std.hypermodel.build(best_hps_std)\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "best_lstm_model_std.fit(X_standard_train_lstm, y_standard_train_lstm, epochs=50, callbacks=[early_stopping], verbose=0)\n",
    "best_lstm_model_std.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ae6fad7-52f6-429c-a52f-24399a529c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 3s 7ms/step - loss: 2.8821e-05\n",
      "mse_standard_train_lstm_scaled: 2.882141234294977e-05\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.2378e-05\n",
      "mse_standard_test_lstm_scaled: 3.237785131204873e-05\n"
     ]
    }
   ],
   "source": [
    "# calcuate the MSE for training set and testing set on scaled data\n",
    "mse_standard_train_lstm_scaled = best_lstm_model_std.evaluate(X_standard_train_lstm, y_standard_train_lstm)\n",
    "print('mse_standard_train_lstm_scaled:', mse_standard_train_lstm_scaled)\n",
    "mse_standard_test_lstm_scaled = best_lstm_model_std.evaluate(X_standard_test_lstm, y_standard_test_lstm)\n",
    "print('mse_standard_test_lstm_scaled:', mse_standard_test_lstm_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b62bc-3d8a-455c-ae52-f56c237d89be",
   "metadata": {},
   "source": [
    "__*We can exclude the possibility of overfitting*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "114e573b-1c2c-4e78-b473-0d0152dffd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 7ms/step\n",
      "mse_standard_test_lstm = 12002.308474647894\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE of the test set on the orignal scale\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_predicted_standard_test_lstm = standardscaler.inverse_transform(best_lstm_model_std.predict(X_standard_test_lstm))\n",
    "y_predicted_standard_test_lstm = y_predicted_standard_test_lstm.reshape(y_predicted_standard_test_lstm.shape[0])\n",
    "y_true_test = raw.loc[(raw['month'] == 12) & (raw['year'] == 2021)]['death'].values\n",
    "mse_standard_test_lstm = mean_squared_error(y_true_test, y_predicted_standard_test_lstm)\n",
    "print('mse_standard_test_lstm =', mse_standard_test_lstm) #unbiased point estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e42298d6-46ac-4b3a-a1c6-8d43c0c18858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3917\n",
      "0\n",
      "117.51195\n",
      "1399\n"
     ]
    }
   ],
   "source": [
    "print(min(y_predicted_standard_test_lstm))\n",
    "print(min(y_true_test))\n",
    "print(max(y_predicted_standard_test_lstm))\n",
    "print(max(y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddacc62-e384-4e1a-9ba7-cc441ea37e86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd9437-5cac-4426-a4c7-4fdd663b8e24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "634ff172-6771-4375-8cf4-8701d35390fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# split the 'raw' data into training set (2018.01 - 2021.11) and testing set (2021.12)\n",
    "# set the start and end dates for training set\n",
    "start_date_train_arimax = pd.to_datetime('2018-01-01')\n",
    "end_date_train_arimax = pd.to_datetime('2021-11-01')\n",
    "raw_train_arimax = raw[(raw['date'] >= start_date_train_arimax) & (raw['date'] <= end_date_train_arimax)]\n",
    "print(len(raw_train_arimax) == (12*4-1)*county_code_onehot.shape[1])\n",
    "\n",
    "#set the date for testing set\n",
    "date_test_arimax = pd.to_datetime('2021-12-01')\n",
    "raw_test_arimax = raw[(raw['date'] == date_test_arimax)]\n",
    "print(len(raw_test_arimax) == 1*county_code_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b5a985-3397-44b8-aae3-662c9bf6d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death_train_arimax.shape: (15557,)\n",
      "death_test_arimax.shape: (331,)\n"
     ]
    }
   ],
   "source": [
    "# create training data and testing data for 'death'\n",
    "death_train_arimax = raw_train_arimax['death'].to_numpy()\n",
    "print('death_train_arimax.shape:',death_train_arimax.shape)\n",
    "death_test_arimax = raw_test_arimax['death'].to_numpy()\n",
    "print('death_test_arimax.shape:',death_test_arimax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457b245c-b303-4955-a29e-b390862051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "county_code_pca = pca.fit_transform(county_code_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7467512-83e4-4bc0-8e5f-33cb996afd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a county_code, find its pca representation\n",
    "def get_county_code_pca(county_code):\n",
    "    county_code_list = raw['county_code'].tolist()\n",
    "    if county_code in county_code_list:\n",
    "        county_index = county_code_list.index(county_code)\n",
    "        county_encoding = county_code_pca[county_index]\n",
    "        return county_encoding\n",
    "    else:\n",
    "        print('County code not found.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "468a3b6e-c9d2-4a08-a23e-822763ff322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county_code_pca_train_arimax.shape: (15557, 2)\n",
      "county_code_pca_test_arimax.shape: (331, 2)\n"
     ]
    }
   ],
   "source": [
    "# create training data and testing data for county names\n",
    "county_code_train_arimax = raw_train_arimax['county_code']\n",
    "county_code_pca_train_arimax = county_code_train_arimax.map(get_county_code_pca)\n",
    "county_code_pca_train_arimax = np.vstack(county_code_pca_train_arimax)\n",
    "print('county_code_pca_train_arimax.shape:', county_code_pca_train_arimax.shape)\n",
    "\n",
    "county_code_test_arimax = raw_test_arimax['county_code']\n",
    "county_code_pca_test_arimax = county_code_test_arimax.map(get_county_code_pca)\n",
    "county_code_pca_test_arimax = np.vstack(county_code_pca_test_arimax)\n",
    "print('county_code_pca_test_arimax.shape:', county_code_pca_test_arimax.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38702d-3f35-4457-b195-f409145fdb34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit ARIMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214bc3c5-115d-4319-8e9a-b66409a4f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (0, 0, 0) successfully fitted. AIC: 191026.03652221922\n",
      "ARIMA model with parameters (0, 0, 1) successfully fitted. AIC: 175864.82341712218\n",
      "ARIMA model with parameters (0, 0, 2) successfully fitted. AIC: 166716.19832595246\n",
      "ARIMA model with parameters (0, 0, 3) successfully fitted. AIC: 162108.52324548084\n",
      "ARIMA model with parameters (0, 1, 0) successfully fitted. AIC: 150162.1990181963\n",
      "ARIMA model with parameters (0, 1, 1) successfully fitted. AIC: 149635.71695110365\n",
      "ARIMA model with parameters (0, 1, 2) successfully fitted. AIC: 149637.69721823608\n",
      "ARIMA model with parameters (0, 1, 3) successfully fitted. AIC: 149614.01264071662\n",
      "ARIMA model with parameters (0, 2, 0) successfully fitted. AIC: 163521.09508376668\n",
      "ARIMA model with parameters (0, 2, 1) successfully fitted. AIC: 150165.99257663864\n",
      "ARIMA model with parameters (0, 2, 2) successfully fitted. AIC: 149640.19726717687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (0, 2, 3) successfully fitted. AIC: 149641.93474158575\n",
      "ARIMA model with parameters (0, 3, 0) successfully fitted. AIC: 181428.0597283297\n",
      "ARIMA model with parameters (0, 3, 1) successfully fitted. AIC: 163526.93863782496\n",
      "ARIMA model with parameters (0, 3, 2) successfully fitted. AIC: 150197.94379047945\n",
      "ARIMA model with parameters (0, 3, 3) successfully fitted. AIC: 149711.44501491872\n",
      "ARIMA model with parameters (1, 0, 0) successfully fitted. AIC: 149899.74248491885\n",
      "ARIMA model with parameters (1, 0, 1) successfully fitted. AIC: 149469.8519410194\n",
      "ARIMA model with parameters (1, 0, 2) successfully fitted. AIC: 149469.4935677942\n",
      "ARIMA model with parameters (1, 0, 3) successfully fitted. AIC: 149459.87924134193\n",
      "ARIMA model with parameters (1, 1, 0) successfully fitted. AIC: 149646.8382117896\n",
      "ARIMA model with parameters (1, 1, 1) successfully fitted. AIC: 149637.68158298437\n",
      "ARIMA model with parameters (1, 1, 2) successfully fitted. AIC: 149623.37521771598\n",
      "ARIMA model with parameters (1, 1, 3) successfully fitted. AIC: 149604.53172187862\n",
      "ARIMA model with parameters (1, 2, 0) successfully fitted. AIC: 157087.60798148124\n",
      "ARIMA model with parameters (1, 2, 1) successfully fitted. AIC: 149650.9996261667\n",
      "ARIMA model with parameters (1, 2, 2) successfully fitted. AIC: 149957.18576991552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (1, 2, 3) successfully fitted. AIC: 149557.8983868864\n",
      "ARIMA model with parameters (1, 3, 0) successfully fitted. AIC: 170239.8560344665\n",
      "ARIMA model with parameters (1, 3, 1) successfully fitted. AIC: 157092.05131368144\n",
      "ARIMA model with parameters (1, 3, 2) successfully fitted. AIC: 149712.84727037314\n",
      "ARIMA model with parameters (1, 3, 3) successfully fitted. AIC: 150182.60477026924\n",
      "ARIMA model with parameters (2, 0, 0) successfully fitted. AIC: 149467.65508420859\n",
      "ARIMA model with parameters (2, 0, 1) successfully fitted. AIC: 149468.37609491052\n",
      "ARIMA model with parameters (2, 0, 2) successfully fitted. AIC: 149384.9731582981\n",
      "ARIMA model with parameters (2, 0, 3) successfully fitted. AIC: 149460.93966323347\n",
      "ARIMA model with parameters (2, 1, 0) successfully fitted. AIC: 149642.4215351455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (2, 1, 1) successfully fitted. AIC: 149611.63727213783\n",
      "ARIMA model with parameters (2, 1, 2) successfully fitted. AIC: 149604.9734666856\n",
      "ARIMA model with parameters (2, 1, 3) successfully fitted. AIC: 149606.96125101286\n",
      "ARIMA model with parameters (2, 2, 0) successfully fitted. AIC: 154957.0014067062\n",
      "ARIMA model with parameters (2, 2, 1) successfully fitted. AIC: 149646.61153502174\n",
      "ARIMA model with parameters (2, 2, 2) successfully fitted. AIC: 149581.69672015787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (2, 2, 3) successfully fitted. AIC: 149555.42374949978\n",
      "ARIMA model with parameters (2, 3, 0) successfully fitted. AIC: 165306.64720092603\n",
      "ARIMA model with parameters (2, 3, 1) successfully fitted. AIC: 154961.62500972848\n",
      "ARIMA model with parameters (2, 3, 2) successfully fitted. AIC: 149699.64745877092\n",
      "ARIMA model with parameters (2, 3, 3) successfully fitted. AIC: 149702.83446732027\n",
      "ARIMA model with parameters (3, 0, 0) successfully fitted. AIC: 149468.75840923598\n",
      "ARIMA model with parameters (3, 0, 1) successfully fitted. AIC: 149395.84468666228\n",
      "ARIMA model with parameters (3, 0, 2) successfully fitted. AIC: 149458.76239219366\n",
      "ARIMA model with parameters (3, 0, 3) successfully fitted. AIC: 149460.11613134918\n",
      "ARIMA model with parameters (3, 1, 0) successfully fitted. AIC: 149623.353099711\n",
      "ARIMA model with parameters (3, 1, 1) successfully fitted. AIC: 149604.4455835144\n",
      "ARIMA model with parameters (3, 1, 2) successfully fitted. AIC: 149606.85765026556\n",
      "ARIMA model with parameters (3, 1, 3) successfully fitted. AIC: 149608.7930575152\n",
      "ARIMA model with parameters (3, 2, 0) successfully fitted. AIC: 153696.7152801971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (3, 2, 1) successfully fitted. AIC: 149627.21032145835\n",
      "ARIMA model with parameters (3, 2, 2) successfully fitted. AIC: 149562.49981375778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (3, 2, 3) successfully fitted. AIC: 149677.4295148339\n",
      "ARIMA model with parameters (3, 3, 0) successfully fitted. AIC: 162078.31940262235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (3, 3, 1) successfully fitted. AIC: 153700.90669278803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (3, 3, 2) successfully fitted. AIC: 152790.7013755003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonding/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model with parameters (3, 3, 3) successfully fitted. AIC: 149686.44727607747\n",
      "The best ARIMAX parameters are: p=2, d=0, q=2 with AIC=149384.9731582981\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Find the best values of p, d, q\n",
    "p = d = q = range(0, 4) #take any value between 0 and 3\n",
    "pdq = list(itertools.product(p, d, q)) # Generate all different combinations of p, d and q triplets\n",
    "\n",
    "arimax_model_results = []\n",
    "# Find the best parameters\n",
    "for parms in pdq: \n",
    "    try:\n",
    "        model_arimax = ARIMA(death_train_arimax, order = parms, exog = county_code_pca_train_arimax)\n",
    "        model_arimax_fit = model_arimax.fit()\n",
    "        arimax_model_results.append((parms, model_arimax_fit.aic))\n",
    "        print(f\"ARIMA model with parameters {parms} successfully fitted. AIC: {model_arimax_fit.aic}\")\n",
    "        \n",
    "    except:\n",
    "        print(f\"ARIMA model with parameters {parms} failed to fit. Error: {e}\")\n",
    "        continue\n",
    "best_params = min(arimax_model_results, key = lambda x:x[1])\n",
    "print(f\"The best ARIMAX parameters are: p={best_params[0][0]}, d={best_params[0][1]}, q={best_params[0][2]} with AIC={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "522ae677-37d5-4672-bd2b-5dced106baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ARIMAX model AIC:  149384.9731582981\n",
      "Best ARIMAX model BIC:  149446.19128611477\n"
     ]
    }
   ],
   "source": [
    "# Choose the ARIMAX model with best parameters (2,0,2)\n",
    "best_arimax_model = ARIMA(death_train_arimax, exog = county_code_pca_train_arimax, order = (2,0,2))\n",
    "best_arimax_model = best_arimax_model.fit()\n",
    "print(\"Best ARIMAX model AIC: \", best_arimax_model.aic)\n",
    "print(\"Best ARIMAX model BIC: \", best_arimax_model.bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f045761-89ad-4dc0-a29f-6a3d3f71751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of prediction from best ARIMAX model (2,0,2) on testing data = 15013.826618600395\n"
     ]
    }
   ],
   "source": [
    "# Predict using the best ARIMAX model\n",
    "arimax_test_predicted = []\n",
    "for i in range(len(county_code_pca_test_arimax)):\n",
    "    county_code_pca_test_arimax_single_county = county_code_pca_test_arimax[i,:].reshape(1,-1)\n",
    "    arimax_test_predicted_single_county = best_arimax_model.get_forecast(steps = 1, exog = county_code_pca_test_arimax_single_county)\n",
    "    arimax_test_predicted_single_county = arimax_test_predicted_single_county.predicted_mean[0]\n",
    "    arimax_test_predicted.append(arimax_best_predicted_single_county)\n",
    "\n",
    "# calculate the MSE on testing set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_test_arimax = mean_squared_error(death_test_arimax, arimax_test_predicted)\n",
    "print('MSE of prediction from best ARIMAX model (2,0,2) on testing data =', mse_test_arimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eef05c3b-1bd0-4432-8b38-1b4b5d2d76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of prediction from best ARIMAX model (2,0,2) on training data = 865.5383232344526\n"
     ]
    }
   ],
   "source": [
    "# Predict using the best ARIMAX model on training data\n",
    "arimax_train_predicted = best_arimax_model.predict(start=0, end=len(death_train_arimax)-1, exog=county_code_pca_train_arimax)\n",
    "\n",
    "# Calculate the MSE on training data\n",
    "mse_train_arimax = mean_squared_error(death_train_arimax, arimax_train_predicted)\n",
    "print('MSE of prediction from best ARIMAX model (2,0,2) on training data =', mse_train_arimax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09301aa7-4388-4c5c-9c8b-814e4c085d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dafbde92-bda1-42d2-979e-84799712ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exp = raw.copy()\n",
    "# Convert 'date' to datetime format\n",
    "raw_exp['date'] = pd.to_datetime(raw_exp['date'])\n",
    "# Set 'date' as the index\n",
    "raw_exp = raw.set_index('date')\n",
    "# Sort the DataFrame by the index (date)\n",
    "raw_exp = raw_exp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8221d435-5d1a-43b7-a683-e4c6bb3c0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "exp_predicted = {}\n",
    "# loop over each county_code:\n",
    "for county_code in raw_exp['county_code'].unique():\n",
    "    df_county_code = raw_exp[raw_exp['county_code'] == county_code]['death']\n",
    "    df_county_code.index = pd.date_range(start='2018-01-01', end='2021-12-31', freq='M')\n",
    "    df_county_code = df_county_code.sort_index()  # sort dataframe in ascending order of date\n",
    "    df_county_code.index = df_county_code.index.to_period('M')  # set the frequency to monthly\n",
    "\n",
    "    # Split into training and test data\n",
    "    exp_train = df_county_code[:-1]  # all data up to November 2021\n",
    "    exp_test = df_county_code[-1:]   # December 2021\n",
    "    exp_model = ExponentialSmoothing(exp_train, seasonal_periods=12, trend='add', seasonal='add').fit()\n",
    "    # Predict for December 2021\n",
    "    exp_predicted_dec = exp_model.predict(start=len(exp_train), end=len(exp_train)) # len(train) corresponds to the next time step right after the end of the training data\n",
    "    exp_predicted[county_code] = exp_predicted_dec\n",
    "\n",
    "# sort the exp_predicted dictionary by the value of the key\n",
    "exp_predicted = dict(sorted(exp_predicted.items()))\n",
    "for key in exp_predicted:\n",
    "    exp_predicted[key] = exp_predicted[key].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec52848c-898c-4de6-86b1-3efac6206f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the true value of death in Dec 2021 for each county and store them in a dictionary\n",
    "raw_exp_sorted = raw_exp.sort_values('county_code')\n",
    "raw_exp_test = raw_exp_sorted[raw_exp_sorted.index == '2021-12-01']\n",
    "exp_test_true = {}\n",
    "for i in range(len(raw_exp_test)):\n",
    "            exp_test_true[raw_exp_test['county_code'].iloc[i]] = raw_exp_test['death'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3959306-8024-4a11-9efb-8692786b2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of prediction from triple exponential smoothing model = 155.42821982122163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_exp = mean_squared_error(list(exp_test_true.values()), list(exp_predicted.values()))\n",
    "print('MSE of prediction from triple exponential smoothing model =', mse_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b58bc5cf-f84b-4a2e-8382-9fa310b97090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 19.9723761749089\n",
      "0 0.6651774548329896\n",
      "13 10.526315406745335\n",
      "0 0.017434120834306677\n",
      "17 19.104696961356474\n",
      "12 3.789342416418102\n",
      "30 32.280697851459195\n",
      "28 16.403510296867353\n",
      "59 58.368875065857814\n",
      "37 39.05644792818512\n",
      "26 28.70174328750262\n",
      "13 16.10513289059496\n",
      "0 0.1580994164426947\n",
      "109 117.73661085056638\n",
      "1 0.9824548825437034\n",
      "82 71.12956397014455\n",
      "1 4.210525634906256\n",
      "13 9.876964859859235\n",
      "49 38.24559425002614\n",
      "31 16.91215234570568\n",
      "35 29.59656672470758\n",
      "0 4.192981611839382\n",
      "1 0.561402739637874\n",
      "54 30.17545290632733\n",
      "20 20.5934052067256\n",
      "31 26.333395630920545\n",
      "0 1.397499354339277\n",
      "0 5.947368610571766\n",
      "0 0.5438606292221699\n",
      "0 0.4557439582039453\n",
      "0 5.931172196221459\n",
      "12 9.1227865193279\n",
      "13 16.140271274479357\n",
      "0 0.4035072073015743\n",
      "1 6.719306806364809\n",
      "40 30.438678097075528\n",
      "1 12.63143652663782\n",
      "12 8.824553361648977\n",
      "0 0.7368408908658062\n",
      "39 37.92963256897182\n",
      "20 20.561722854455137\n",
      "32 16.315778607075956\n",
      "9 5.701753419361289\n",
      "22 19.85991127483378\n",
      "21 30.070114353799994\n",
      "16 21.350571792652776\n",
      "10 -0.17463873514433947\n",
      "12 8.561323237640448\n",
      "25 25.526325301936765\n",
      "16 14.701753858853387\n",
      "59 50.94516187755492\n",
      "12 9.947813560551538\n",
      "1 2.8669127139612414\n",
      "10 12.315787107718952\n",
      "466 364.38455931274757\n",
      "27 21.52658372577886\n",
      "35 22.385956634145103\n",
      "18 25.97117332012937\n",
      "13 9.526414875330545\n",
      "36 33.31578657961369\n",
      "28 28.403507497680756\n",
      "28 27.034267918274292\n",
      "56 39.43894123544576\n",
      "11 8.947373045094174\n",
      "0 0.7476481614569845\n",
      "40 46.929664856815755\n",
      "15 19.26310587533336\n",
      "23 27.84197711123908\n",
      "32 30.964902111964363\n",
      "11 1.2252182930029123\n",
      "0 0.035093152870841526\n",
      "367 358.06990285649755\n",
      "42 29.051938799855723\n",
      "42 38.701712299768744\n",
      "13 3.9474593178685264\n",
      "1 -0.280736962987959\n",
      "15 17.631550169944745\n",
      "35 33.964711195164035\n",
      "0 9.894522742910066\n",
      "64 56.14064097425658\n",
      "15 21.245624192572244\n",
      "1 1.1929811183551995\n",
      "0 0.4737673434913451\n",
      "17 17.543863791884636\n",
      "9 17.877133242553764\n",
      "0 1.298305992979939\n",
      "0 14.08743123160313\n",
      "39 39.105447348385894\n",
      "1 0.6139943980006639\n",
      "21 15.699304464957972\n",
      "130 141.105683152322\n",
      "670 688.068923160619\n",
      "1 -0.40355209148880283\n",
      "0 -0.28356168136617954\n",
      "14 19.350890017318026\n",
      "49 56.95634454611986\n",
      "129 131.77252774458677\n",
      "54 64.22911857117954\n",
      "0 0.2457649482050385\n",
      "0 0.43863751215181346\n",
      "0 0.5790058708719108\n",
      "41 34.095055659279275\n",
      "18 12.245584017908214\n",
      "28 26.263570779693353\n",
      "26 14.491063142799684\n",
      "12 17.122750188342188\n",
      "11 11.824532895081715\n",
      "147 148.14033478569394\n",
      "0 11.94736486122768\n",
      "0 0.3220714115397221\n",
      "27 18.996311635485874\n",
      "0 2.052652910402975\n",
      "10 22.649202065156377\n",
      "30 35.153499251081996\n",
      "0 3.350861840576015\n",
      "11 1.7720888989783168\n",
      "0 0.6797496399969509\n",
      "1 0.8948655119292037\n",
      "0 8.315795484240859\n",
      "315 267.8922802330816\n",
      "0 0.421032950532489\n",
      "23 22.140377228064203\n",
      "65 55.94702323606062\n",
      "10 5.0877179195147555\n",
      "1 0.5087275103195267\n",
      "28 20.718969895007877\n",
      "20 34.52623709040491\n",
      "1 0.05265537619980931\n",
      "1 0.10037025861687873\n",
      "1 0.33350605920734033\n",
      "0 3.069949512919494\n",
      "0 1.3442796901435268\n",
      "0 0.9998948540499357\n",
      "850 936.6470774171449\n",
      "11 4.052872071539994\n",
      "14 8.842100296762903\n",
      "1 0.6666313875986192\n",
      "256 237.17604241620262\n",
      "14 11.228087657226999\n",
      "0 0.2631972758176879\n",
      "0 1.7719476460903176\n",
      "0 0.43863611835951455\n",
      "0 5.701916825761662\n",
      "15 21.456136690667783\n",
      "72 55.21852658554525\n",
      "0 1.0877108687416548\n",
      "68 75.89472981745898\n",
      "311 290.30322488971575\n",
      "16 22.754387429377054\n",
      "11 8.9123449366098\n",
      "26 30.824559829692085\n",
      "13 23.35087474979049\n",
      "0 0.035118769634917946\n",
      "0 0.6138699620484919\n",
      "0 0.5789467827424389\n",
      "175 206.59196517447393\n",
      "1 4.157945735319412\n",
      "11 11.402882184758063\n",
      "11 11.509930846703956\n",
      "11 11.508763671943587\n",
      "126 157.75971558387502\n",
      "0 0.244961961212528\n",
      "30 18.684689861194922\n",
      "0 0.035011342339281215\n",
      "11 0.2984023550879832\n",
      "9 16.210627949991576\n",
      "21 23.756449955598065\n",
      "89 90.43823444073146\n",
      "82 76.2820257468797\n",
      "17 18.754384151805596\n",
      "65 65.52180632367393\n",
      "30 18.350897833886354\n",
      "0 0.7476953304007652\n",
      "1 16.53512228336526\n",
      "0 4.263104535510531\n",
      "1 0.3957951074086695\n",
      "37 29.683767740373828\n",
      "1399 1487.2379273706788\n",
      "39 35.33971786920314\n",
      "0 0.4736823316639433\n",
      "0 5.84210205955722\n",
      "63 68.5791460850795\n",
      "1 0.8421045159069253\n",
      "60 87.2434166156916\n",
      "276 296.06521796844436\n",
      "32 27.75429776452706\n",
      "24 20.544248944516816\n",
      "56 46.631498258053156\n",
      "28 21.263160808129484\n",
      "11 21.91169868327475\n",
      "29 22.130988651436667\n",
      "0 0.5437973549122095\n",
      "65 57.893909833594776\n",
      "19 13.192734991265468\n",
      "0 0.8070167818265278\n",
      "1 6.701318889392102\n",
      "1 7.018886628951746\n",
      "25 27.10544094405582\n",
      "1 0.28069758609937084\n",
      "134 140.42504673744264\n",
      "1 0.6140350616219006\n",
      "30 26.943283354068065\n",
      "86 91.45397350468947\n",
      "14 8.035760284293904\n",
      "10 8.666626750450781\n",
      "64 58.52778033780101\n",
      "16 19.456509155012064\n",
      "0 0.24563188017592585\n",
      "0 0.1930076067109125\n",
      "47 30.315632603966435\n",
      "0 0.45612896001692943\n",
      "0 -0.14035161351695863\n",
      "1 0.12267832964067427\n",
      "10 17.684218302079845\n",
      "1 0.8771929141175949\n",
      "39 35.64921066626713\n",
      "18 5.421303865339288\n",
      "14 11.105353065448252\n",
      "0 1.298138766088691\n",
      "13 14.157899706699816\n",
      "0 17.192990981522648\n",
      "1 14.492316395719095\n",
      "40 44.83648035575365\n",
      "19 18.929822180073316\n",
      "0 0.08758877487899869\n",
      "0 6.035201512929831\n",
      "25 21.228133426092562\n",
      "0 0.03508695127361506\n",
      "177 160.3325212486784\n",
      "0 0.8947353911141989\n",
      "0 1.1578945588058587\n",
      "127 137.0539129360527\n",
      "0 0.31578868634766033\n",
      "0 8.666373388948745\n",
      "12 10.999665548532143\n",
      "1 0.38148903819952695\n",
      "0 0.43864585607999146\n",
      "16 22.85998741322768\n",
      "30 28.94737939464668\n",
      "32 22.842105859529607\n",
      "0 0.31578779104688365\n",
      "60 49.060461961269226\n",
      "18 21.18189127076463\n",
      "1 0.5614026766854305\n",
      "0 5.403505370738308\n",
      "22 21.98245455320876\n",
      "169 212.5623919606441\n",
      "10 6.713760483508839\n",
      "1 10.35086907954964\n",
      "0 0.19298045169896694\n",
      "43 28.732967384088546\n",
      "38 30.66666108299555\n",
      "9 9.525661700825829\n",
      "9 5.894702161467246\n",
      "163 147.40350864441302\n",
      "0 14.114295896663311\n",
      "1 0.28069922915992473\n",
      "47 42.91239579873425\n",
      "24 23.3683479894523\n",
      "13 21.175426441341077\n",
      "82 77.83657739260363\n",
      "0 1.4736393107372234\n",
      "0 3.494012023231059\n",
      "39 36.85962508059615\n",
      "69 78.03072015347036\n",
      "9 0.4736834994716529\n",
      "12 7.754390412095917\n",
      "81 51.57896846679551\n",
      "0 0.43859453777878743\n",
      "1 0.26342204562197336\n",
      "12 12.45610839336044\n",
      "10 5.172575842241229\n",
      "1 1.122806240647973\n",
      "0 0.4914727555497234\n",
      "15 9.245969579062727\n",
      "45 38.72300709960272\n",
      "0 6.526316993445976\n",
      "19 36.368419460283896\n",
      "10 13.96510238086644\n",
      "1 11.824560686444729\n",
      "15 18.000013780404736\n",
      "30 26.824560154950277\n",
      "1 0.2806979298374115\n",
      "1 0.5087760010894307\n",
      "20 8.1186763136689\n",
      "0 0.33342559166453983\n",
      "16 18.63157600838029\n",
      "1 0.30878693590517176\n",
      "156 127.64593138738778\n",
      "0 1.438579726784564\n",
      "22 31.824674972189616\n",
      "10 10.38598882811069\n",
      "0 0.2982391341907963\n",
      "1 0.3507957084691451\n",
      "0 0.5789140541202462\n",
      "11 3.7896641152934087\n",
      "771 785.4366869889864\n",
      "86 77.80737917212494\n",
      "0 0.21053337845208525\n",
      "12 -0.9542862928027915\n",
      "0 0.7895484348818016\n",
      "18 13.632921545249609\n",
      "59 58.48265597646434\n",
      "272 312.99625883545355\n",
      "12 22.74907105704837\n",
      "17 20.19300213961587\n",
      "37 25.754151953212137\n",
      "1 0.25378665761477287\n",
      "12 21.578743088806032\n",
      "28 20.8070167772629\n",
      "43 39.33371917991402\n",
      "48 50.35101275480035\n",
      "29 29.177502164776268\n",
      "15 19.807016023392872\n",
      "0 3.376366826791581\n",
      "17 28.53828751123107\n",
      "87 98.57917268250796\n",
      "26 26.438682607874235\n",
      "0 0.5085535858907597\n",
      "93 86.64587498953392\n",
      "12 15.982451921808272\n",
      "1 11.263011191609992\n",
      "176 143.5257126871727\n",
      "26 25.736841403182844\n",
      "1 11.057148284212818\n",
      "31 37.24542054995057\n",
      "30 33.315811243223926\n",
      "0 3.6666712530048127\n",
      "19 18.438599049319414\n",
      "1 3.0877513897522224\n",
      "1 6.824572485700282\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(list(exp_test_true.values()), list(exp_predicted.values())):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cca720-d6f6-4178-ab8d-9ace2f76afd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
